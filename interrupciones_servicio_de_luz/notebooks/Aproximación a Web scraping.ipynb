{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad498d85",
   "metadata": {},
   "source": [
    "# Obtener información a través del HTML\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "A continuación, vamos a ver cómo podemos usar Beautiful Soup para obtener una cierta información de una página web. Aquí, por ejemplo, vamos a ir a buscar la información correspondiente a las referencias externas. Vamos a intentar obtener la información que aparece aquí y la que aparece aquí. Más concretamente conseguir los enlaces para ir siguiendo todas estas páginas web. Para hacerlo, vamos a cargar esta página, vamos a crear el 'soup' y vamos a mostrar la estructura en árbol. Lo primero que vamos a intentar, y no todo este código va a funcionar correctamente, es encontrar los enlaces externos, que es el título bajo el cual se encontraban algunos de los enlaces. Aquí sencillamente nos muestra el título. Esto no es lo que estábamos buscando. Así que vamos a ir a buscar todos los enlaces. Vamos a encontrar todos aquellos que cuenten con enlaces externos. Y vemos que volvemos a encontrarnos con lo mismo. Podemos usar las funciones de encontrar el elemento siguiente para ver cómo funciona y obtenemos esta información de aquí. Esto es un enlace, pero usaré esta estructura de aquí. Podríamos usarla para ir iterando sobre cuál es el elemento siguiente. Pero no termina de ser la manera más elegante de buscar toda la información que nos interesa. Otra opción que propongo es explorar la función contenido, y vemos que el contenido es sencillamente toda esta parte de aquí que no es código de HTML, que es exactamente esto. Estas aproximaciones que usan las funciones más básicas de Beautiful Soup son bastante más limitadas que realizar un análisis un poco más en profundidad. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "293ada0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cortes = soup.find(id=\"CortesProgramados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c3a9e5d-e0c5-433d-bedf-d0a14170dd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"table table-hover\" id=\"CortesProgramados\">\n",
       "<caption style=\"display: none;\">Cortes Programados por Mantenimiento y Obras</caption>\n",
       "<div class=\"form-group\" style=\"display: none;\"><input class=\"form-control search\" placeholder=\"Buscar Partido\"/></div>\n",
       "<thead>\n",
       "<tr class=\"bg-info\">\n",
       "<th>PARTIDO</th>\n",
       "<th>LOCALIDAD / BARRIO</th>\n",
       "<th>SUBESTACION / ALIMENTADOR</th>\n",
       "<th>USUARIOS AFECTADOS</th>\n",
       "<th>HORA ESTIMADA DE NORMALIZACION</th>\n",
       "</tr> </thead>\n",
       "<tbody class=\"list\">\n",
       "<tr class=\"text-center\" v-if=\"sinProgramados\">\n",
       "<td colspan=\"5\">Sin Interrupciones por Mantenimiento y Obras</td>\n",
       "</tr>\n",
       "<tr v-for=\"corte in cortesProgramados\">\n",
       "<td class=\"partido\">{{corte.partido}}</td>\n",
       "<td class=\"localidad\">{{corte.localidad}}</td>\n",
       "<td>{{corte.subestacion_alimentador}}</td>\n",
       "<td>{{corte.usuarios}}</td>\n",
       "<td>{{corte.normalizacion}}</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fe872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"Enlaces_externos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces = soup.find(id=\"Enlaces_externos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25911337",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces.find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a899a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces = soup.findAll(\"div\",{\"class\":\"listaref\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752e15c",
   "metadata": {},
   "source": [
    "Es lo que te propongo a continuación. Primero vamos a ver cómo podemos encontrar todos aquellos elementos que estén categorizados con esta etiqueta y que pertenezcan a una cierta clase. En este caso, el listado de referencias. Aquí obtenemos algo que tiene bastante mejor pinta que lo que estábamos obteniendo. Pero, aún así, si lo que nos interesa son los enlaces, aquí aún hay demasiado código. Por ejemplo, vemos que bajo esta etiqueta tenemos enlaces, que es lo que nos interesa conseguir, pero aún hay la referencia, dónde la cita, también hay el nombre y avisos, por ejemplo. Todo esto es información que no queremos, y lo hemos obtenido con esta instrucción de aquí. Y lo que vamos a ver es otro método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6ec14",
   "metadata": {},
   "source": [
    "Vamos a ver este de aquí primero. Aquí estamos consiguiendo todos los objetos que cuelguen de esta etiqueta de aquí. Esto he ido a explorarlo usando el inspeccionar en la página web original. No son etiquetas que sean globales, sino que esto depende en concreto de cómo está estructurada la Wikipedia en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in soup.find(\"div\", attrs={\"class\":\"listaref\"}).descendants:\n",
    "    if d.name == \"span\" and d.get(\"class\", \"\") == [\"reference-text\"]:\n",
    "        print(d.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb631a3",
   "metadata": {},
   "source": [
    " Y este código de aquí es más elaborado y lo que estamos encontrando es cada uno de los elementos que encuentre nuestra etiqueta que cumplan, que sean de la clase lista de referencias, buscar todos sus descendientes. Una vez dentro de esta lista, por cada uno de estos elementos vamos a buscar aquellos cuyo nombre sea este, 'span', y que tengan esta clase, que pertenezcan a este grupo. Y vamos a imprimirlos. Aquí ya hemos obtenido algo mucho más útil. Estamos en el apartado de referencias, pero solo estamos obteniendo los \"links\" de aquellas páginas que directamente tenían el \"link\" en la página web. Esto eran enlaces y aquí no los estamos obteniendo. Solamente estamos obteniendo aquellos que directamente estaban escritos. Es decir, este código funciona correctamente, estamos obteniendo la información de las referencias, pero estamos obteniendo directamente el título de la referencia, no el enlace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eef5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find(\"div\", attrs={\"class\":\"listaref\"})\n",
    "for div in data:\n",
    "    links = div.findAll(\"a\")\n",
    "    for a in links:\n",
    "        if a[\"href\"].startswith(\"http\"):\n",
    "            print(a[\"href\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61f3e0",
   "metadata": {},
   "source": [
    " Y para eso os he puesto aquí un último código, que lo que obtenemos es todos los datos que estén etiquetados así, que pertenezcan a esta clase y, para cada una de estas, vamos a encontrar todos los que estén categorizados con 'a'. 'a' normalmente va asociado a 'links', es decir, enlaces. Y por cada uno de los que haya encontrado aquí, por cada 'a' que encuentre en este objeto, si la hiperreferencia de 'a' empieza con 'http', vamos a imprimirlo. Este código puede parecer muy complicado, pero en realidad solo estamos explorando cada uno de los elementos y, dentro de cada uno de los elementos, cada uno de los elementos. Si ejecutamos esto, obtenemos exactamente lo que estábamos buscando, que es todos los enlaces para cada una de las referencias que teníamos en la página web original. Este tipo de código de aquí está combinando funciones propias de Beautiful Soup con \"loops\" mediante los cuales vamos explorando todos los objetos que nos devuelven estas funciones. Puede parecer muy complejo al principio porque estás trabajando con objetos que son bastante distintos a los que estás acostumbrado con Python. Pero en realidad todo puede explorarse como si fuesen listas. Y ya ves que no estoy utilizando más que dos funciones en particular. Todo lo demás son tratamiento de \"strings\", bucles y condicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d85aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find(\"div\", attrs={\"class\":\"listaref\"})\n",
    "#link = data.findAll(\"a\")\n",
    "for div in data:\n",
    "    links = div.findAll(\"a\")\n",
    "    for a in links:\n",
    "        if a[\"href\"].startswith(\"http\"):\n",
    "            print(a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe61a3",
   "metadata": {},
   "source": [
    "Y para eso os he puesto aquí un último código, que lo que obtenemos es todos los datos que estén etiquetados así, que pertenezcan a esta clase y, para cada una de estas, vamos a encontrar todos los que estén categorizados con 'a'. 'a' normalmente va asociado a 'links', es decir, enlaces. Y por cada uno de los que haya encontrado aquí, por cada 'a' que encuentre en este objeto, si la hiperreferencia de 'a' empieza con 'http', vamos a imprimirlo. Este código puede parecer muy complicado, pero en realidad solo estamos explorando cada uno de los elementos y, dentro de cada uno de los elementos, cada uno de los elementos. Si ejecutamos esto, obtenemos exactamente lo que estábamos buscando, que es todos los enlaces para cada una de las referencias que teníamos en la página web original. Este tipo de código de aquí está combinando funciones propias de Beautiful Soup con \"loops\" mediante los cuales vamos explorando todos los objetos que nos devuelven estas funciones. Puede parecer muy complejo al principio porque estás trabajando con objetos que son bastante distintos a los que estás acostumbrado con Python. Pero en realidad todo puede explorarse como si fuesen listas. Y ya ves que no estoy utilizando más que dos funciones en particular. Todo lo demás son tratamiento de \"strings\", bucles y condicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af5d87",
   "metadata": {},
   "source": [
    "---\n",
    "# Extracción de texto usando Beautiful Soup\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Si quieres extraer información textual de una página web para posteriormente realizar un análisis de \"text mining\", en este vídeo voy a mostrarte distintas maneras de cómo puedes hacerlo. Lo primero que tendremos que hacer es cargar toda la información del HTML realizando estas dos funciones y vamos a visualizar qué tipo de información tenemos. Tenemos la estructura en árbol del HTML de la Wikipedia en la entrada de Python. Y aquí vamos a intentar extraer todo lo que corresponde al cuerpo del artículo, es decir, al texto en sí. Una primera aproximación es usar este tipo de estructuras. Por ejemplo, voy a usar el 'sub', voy a extraerle el título y convertirlo en 'string'. Esto funciona correctamente, pero esto solamente es el título. Beautiful Soup nos ofrece esa función que es bastante útil, pero no funciona todo lo exhaustivamente que querríamos. Efectivamente coge texto, pero también nos incorpora algunas cosas que no nos interesan. Por ejemplo, esto claramente no es el texto que estábamos buscando. Más adelante sí que hay texto. Nos incluye también el índice, que no es especialmente interesante si lo que queremos es información textual para procesar. Y aquí ya sí que tenemos más información como la que nos interesa, pero aún hay elementos que no nos interesan. Otra aproximación es ir a buscar el cuerpo del artículo. Aquí tenemos directamente todo el cuerpo con el código asociado. Y podríamos intentar explorar, por ejemplo, los elementos categorizados con 'p'. Es lo que vemos aquí abajo. Obtenemos información. Ciertamente esto es la información que estamos buscando procesar, pero aún no está limpia. Requeriría aún mucho procesamiento. Vamos a hacerlo guardando esta instrucción de aquí. Todo lo que encuentres dentro del cuerpo, todos los elementos categorizados como 'p' y vamos a ver qué tenemos aquí. Tenemos en una lista cada uno de los elementos donde el programa ha detectado que tenemos un párrafo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos = soup.find(\"body\").find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpiarhtml(html):\n",
    "    expresion = re.compile('<.*?>')\n",
    "    texto = re.sub(expresion, \"\", html)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83987255",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in elementos:\n",
    "    print(limpiarhtml(str(elem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e031e6b",
   "metadata": {},
   "source": [
    "Vamos a importar un nuevo paquete y a definir una función que va a limpiarnos todo lo que es código HTML. ¿Cómo se limpia el código HTML? Es muy sencillo. Tenemos que decirle que todo lo que encuentre entre estas flechitas nos lo extraiga. Como es una página web, todo esto va a eliminarlo. En principio, no tendría que haber ningún problema, porque una página web no compilaría si uno de estos está abierto pero no cerrado. Así que, lo que vamos a hacer es pedirle que nos extraiga esto y lo sustituya por nada y directamente nos devuelva el texto limpio. Y aquí sencillamente tenemos que ejecutar esta función sobre cada uno de los elementos que hayamos encontrado usando esta función de aquí. Vamos a ver el resultado. Esto es exactamente lo que estábamos buscando, es decir, toda la información textual lista para ser procesada con un modelo. Es importante notar que he tenido que convertir esto a un 'string'. Si no lo hago, Python se va a quejar. Estaba esperando un 'string' y tengo otro tipo de objeto. Es tan sencillo como añadirle aquí esta conversión. El código para obtener la información textual, como has visto, no es especialmente complicado. Sencillamente es darse cuenta de dónde se encuentra, bajo qué etiqueta y definir una función que puedes usarla, y es muy sencilla de crear, para limpiar todo aquello que no nos interesa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23b7ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Obtención de información no textual\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Una de las vertientes del \"web scraping\" que más interesantes me parecen personalmente es la obtención de \"links\" para explorar cómo están conectadas las páginas entre ellas. Aquí lo que te voy a mostrar son distintas maneras de obtener estos \"links\" con códigos bastante sencillos. Voy a estar usando esta página web, la entrada de la Wikipedia de Python, y vamos a crear un 'soup' directamente. Y vamos a ver cómo obtener todos los \"links\" de una manera muy sencilla. Primero es, dentro del 'soup', encontrar todos los elementos que estén categorizados con 'a' y que tengan atributo 'hiperreferencia', que cumplan que empieza por 'http'.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40692f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276460e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.findAll(\"a\", attrs={\"href\": re.compile(\"^http://\")}):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca7588",
   "metadata": {},
   "source": [
    " ¿Qué estamos obteniendo con esto? No estamos obteniendo todos los \"links\", estamos obteniendo todos los \"links\" que apunten hacia fuera de la página web. Vamos a ver exactamente qué es lo que estamos obteniendo. Y son todos los enlaces que llevan a otras páginas. Aquí no hay entradas de la Wikipedia. Esto es una primera aproximación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8daa223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pagina = requests.get(\"https://www.enre.gov.ar/paginacorte/tabla_cortes_edenor.html\")\n",
    "soup = BeautifulSoup(pagina.text, \"html.parser\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())\n",
    "#list(soup.children)\n",
    "#list(soup_.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06fcf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla =soup.find(\"table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c4e6446-b706-4244-a5a1-76ea6aeb77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Accept': '*/*',\n",
    "    'Accept-Language': 'es-419,es;q=0.9',\n",
    "    'Connection': 'keep-alive',\n",
    "    'If-Modified-Since': 'Mon, 12 Feb 2024 16:42:44 GMT',\n",
    "    'Referer': 'https://www.enre.gov.ar/paginacorte/tabla_cortes_edenor.html',\n",
    "    'Sec-Fetch-Dest': 'script',\n",
    "    'Sec-Fetch-Mode': 'no-cors',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-GPC': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "    'sec-ch-ua': '\"Not A(Brand\";v=\"99\", \"Brave\";v=\"121\", \"Chromium\";v=\"121\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "}\n",
    "\n",
    "params = ''\n",
    "\n",
    "response = requests.get('https://www.enre.gov.ar/paginacorte/js/data_EDN.js', params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69d7acdb-6468-4836-a6c4-b5d28760aed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application/x-javascript'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "98f28248-5f28-45d3-939d-270d3f1e175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpiarhtml(html, exp_reg):\n",
    "    expresion = re.compile('(?:' + exp_reg +')')\n",
    "    texto = re.sub(expresion, \"\", html)\n",
    "    return texto\n",
    "\n",
    "#tabla = limpiarhtml(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "443da95e-dc53-47c2-98d4-c09121ee2917",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m js\u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtabla\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "js= json.dump(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1e93fb53-900a-4d23-b69e-9002ba043369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tfuente : 'Web Service',\n",
      "    empresa: 'EDENOR',\n",
      "    totalUsuariosSinSuministro: '3.365',\n",
      "    totalUsuariosConSuministro: '3.306.845',\n",
      "    ultimaActualizacion: '13:55',\n",
      "    totalUsuariosAyer: '32.373',\n",
      "    cortesPreventivos:  [],\n",
      "    cortesProgramados:  [{partido: 'GRAL SAN MARTIN',localidad: 'C LIBERTADOR SAN MART?N',subestacion_alimentador: '- / R:MALAVER / 68-TR2 / 68-5535',usuarios: '424',normalizacion: '2024-02-12 16:41'},{partido: 'SAN FERNANDO',localidad: 'DELTA 3RA SECCION (SF)',subestacion_alimentador: '- / ',usuarios: '2',normalizacion: 'Sin datos'}],\n",
      "    cortesServicioMedia:  [{partido: 'ESCOBAR',localidad: 'ESCOBAR',subestacion_alimentador: '- / R:MATANZA / 65-TR1 / 65-5517',usuarios: '23',normalizacion: 'Sin datos'},{partido: 'MORENO',localidad: 'TRUJUI',subestacion_alimentador: '- / R:CATONAS / 256-TR1 / 256-5541',usuarios: '851',normalizacion: '2024-02-12 14:00'},{partido: 'PILAR',localidad: 'PILAR',subestacion_alimentador: '- / R:MANZONE / 257-TR2 / 257-5523',usuarios: '418',normalizacion: 'Sin datos'},{partido: 'PILAR',localidad: 'VILLA ROSA',subestacion_alimentador: '- / R:DERQUI / 260-TR1 / 260-5511',usuarios: '1',normalizacion: 'Sin datos'},{partido: 'SAN MIGUEL',localidad: 'BELLA VISTA',subestacion_alimentador: '- / R:CATONAS / 256-TR1 / 256-5541',usuarios: '1030',normalizacion: '2024-02-12 14:00'},{partido: 'TIGRE',localidad: 'CIUDAD DE TIGRE',subestacion_alimentador: '- / R:TIGRE / 153-TR1 / 153-5517',usuarios: '196',normalizacion: '2024-02-12 14:00'}],\n",
      "    cortesComunicados:  [],\n",
      "    cortesServicioBaja:  [ {partido: '3 DE FEBRERO', localidad: 'CASEROS', usuarios: '8'}, {partido: '3 DE FEBRERO', localidad: 'CIUDADELA', usuarios: '1'}, {partido: 'CAPITAL FEDERAL', localidad: 'PALERMO', usuarios: '7'}, {partido: 'ESCOBAR', localidad: 'GARIN', usuarios: '29'}, {partido: 'ESCOBAR', localidad: 'INGENIERO MASCHWITZ', usuarios: '1'}, {partido: 'GRAL LAS HERAS', localidad: 'GRAL LAS HERAS', usuarios: '4'}, {partido: 'GRAL RODRIGUEZ', localidad: 'GRAL RODRIGUEZ', usuarios: '24'}, {partido: 'GRAL SAN MARTIN', localidad: 'VILLA BALLESTER', usuarios: '9'}, {partido: 'ITUZAINGO', localidad: 'ITUZAINGO', usuarios: '4'}, {partido: 'LA MATANZA', localidad: 'GONZALEZ CATAN', usuarios: '17'}, {partido: 'LA MATANZA', localidad: 'ISIDRO CASANOVA', usuarios: '14'}, {partido: 'LA MATANZA', localidad: 'SAN JUSTO', usuarios: '54'}, {partido: 'LA MATANZA', localidad: 'VILLA LUZURIAGA', usuarios: '18'}, {partido: 'LA MATANZA', localidad: 'VILLA MADERO', usuarios: '38'}, {partido: 'MERLO', localidad: 'LIBERTAD', usuarios: '19'}, {partido: 'PILAR', localidad: 'MANUEL ALBERTI', usuarios: '5'}, {partido: 'SAN FERNANDO', localidad: 'SAN FERNANDO', usuarios: '17'}, {partido: 'SAN ISIDRO', localidad: 'SAN ISIDRO', usuarios: '26'}, {partido: 'TIGRE', localidad: 'DELTA 1RA SECCION (TI)', usuarios: '67'}, {partido: 'TIGRE', localidad: 'EL TALAR', usuarios: '8'}, {partido: 'VICENTE LOPEZ', localidad: 'FLORIDA', usuarios: '25'}, {partido: 'VICENTE LOPEZ', localidad: 'OLIVOS', usuarios: '25'}]\n",
      "  }\t\t\n",
      "\t\t\n"
     ]
    }
   ],
   "source": [
    "print(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ed9937fe-6fb7-4875-bb9e-3de91ddddea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtabla\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "dict(str(tabla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf3fdb91-10d3-49e9-94c5-ffdd783899dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PARTIDO</th>\n",
       "      <th>LOCALIDAD / BARRIO</th>\n",
       "      <th>SUBESTACION / ALIMENTADOR</th>\n",
       "      <th>USUARIOS AFECTADOS</th>\n",
       "      <th>HORA ESTIMADA DE NORMALIZACION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Previstos Por La Distribuidora</td>\n",
       "      <td>No Previstos Por La Distribuidora</td>\n",
       "      <td>No Previstos Por La Distribuidora</td>\n",
       "      <td>No Previstos Por La Distribuidora</td>\n",
       "      <td>No Previstos Por La Distribuidora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{{corte.partido}}</td>\n",
       "      <td>{{corte.localidad}}</td>\n",
       "      <td>{{corte.subestacion_alimentador}}</td>\n",
       "      <td>{{corte.usuarios}}</td>\n",
       "      <td>{{corte.normalizacion}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             PARTIDO                 LOCALIDAD / BARRIO  \\\n",
       "0  No Previstos Por La Distribuidora  No Previstos Por La Distribuidora   \n",
       "1                  {{corte.partido}}                {{corte.localidad}}   \n",
       "\n",
       "           SUBESTACION / ALIMENTADOR                 USUARIOS AFECTADOS  \\\n",
       "0  No Previstos Por La Distribuidora  No Previstos Por La Distribuidora   \n",
       "1  {{corte.subestacion_alimentador}}                 {{corte.usuarios}}   \n",
       "\n",
       "      HORA ESTIMADA DE NORMALIZACION  \n",
       "0  No Previstos Por La Distribuidora  \n",
       "1            {{corte.normalizacion}}  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_html(str(tabla))[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c32deb-aece-4409-97a3-7fb6e3b6bbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cc59948",
   "metadata": {},
   "source": [
    "## Encabezados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d63ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"      [{partido: 'GRAL SAN MARTIN',localidad: 'C LIBERTADOR SAN MART?N',subestacion_alimentador: '- / R:MALAVER / 68-TR2 / 68-5535',usuarios: '424',normalizacion: '2024-02-12 16:41'},{partido: 'SAN FERNANDO',localidad: 'DELTA 3RA SECCION (SF)',subestacion_alimentador: '- / ',usuarios: '2',normalizacion: 'Sin datos'}],\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limpio el texto\n",
    "tabla_aux = tabla.replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "#Armo una lista de los elementos.\n",
    "tabla_aux= tabla_aux.split(\"\\n\")\n",
    "\n",
    "#Guardo las categorías\n",
    "for i in tabla_aux:\n",
    "    if \"cortesProgramados:\" in i:\n",
    "        cortesProgramados =  i[len(\"cortesProgramados:\"):-1]\n",
    "        expresion = re.compile('(?:cortesProgramados:)')\n",
    "        texto = re.sub(expresion, \"\", i)\n",
    "    \n",
    "\n",
    "texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1cd5a91f-afe4-4907-ab0e-4511b07de38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"      [{partido: 'GRAL SAN MARTIN',localidad: 'C LIBERTADOR SAN MART?N',subestacion_alimentador: '- / R:MALAVER / 68-TR2 / 68-5535',usuarios: '424',normalizacion: '2024-02-12 16:41'},{partido: 'SAN FERNANDO',localidad: 'DELTA 3RA SECCION (SF)',subestacion_alimentador: '- / ',usuarios: '2',normalizacion: 'Sin datos'}],\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoria = \"cortesProgramados:\"\n",
    "for i in tabla_aux:\n",
    "    if categoria in i:\n",
    "        texto_dos = limpiarhtml(i, categoria)\n",
    "        \n",
    "texto_dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52bd1713-a37e-4a10-a3cd-666fcc344f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dos:  [{partido: 'GRAL SAN MARTIN',localidad: 'C LIBERTADOR SAN MART?N',subestacion_alimentador: '- / R:MALAVER / 68-TR2 / 68-5535',usuarios: '424',normalizacion: '2024-02-12 16:41'},{partido: 'SAN FERNANDO',localidad: 'DELTA 3RA SECCION (SF)',subestacion_alimentador: '- / ',usuarios: '2',normalizacion: 'Sin datos'}]\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortesProgramados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c55977-f3f3-4bd8-b761-79ae786dd809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "415e625f",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = tabla_IDH.find_all(\"td\")\n",
    "lista_datos = [i.get_text()[:-1] if \"\\n\" in i.get_text() else i.get_text() for i in aux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = np.array(lista_datos)\n",
    "valores.shape = (20, len(encabezado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf73b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Puesto = [i[0] for i in valores]\n",
    "Puesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(valores, index=Puesto, columns=encabezado)\n",
    "del df[\"Puesto\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IDH según datos del 2021\"] = df[\"IDH según datos del 2021\"].apply(str.strip)\n",
    "df[\"IDH según datos del 2021\"] = df[\"IDH según datos del 2021\"].str.replace(\",\", \".\")\n",
    "df[\"IDH según datos del 2021\"] = df[\"IDH según datos del 2021\"].astype(\"float64\")\n",
    "df[\"IDH según datos del 2019\"] = df[\"IDH según datos del 2019\"].apply(str.strip)\n",
    "df[\"IDH según datos del 2019\"] = df[\"IDH según datos del 2019\"].str.replace(\",\", \".\")\n",
    "df[\"IDH según datos del 2019\"] = df[\"IDH según datos del 2019\"].astype(\"float64\")\n",
    "df[\"Crecimiento\"] = df[\"Crecimiento\"].apply(str.strip)\n",
    "df[\"Crecimiento\"] = df[\"Crecimiento\"].str.replace(\",\", \".\")\n",
    "df[\"Crecimiento\"] = df[\"Crecimiento\"].astype(\"float64\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6160c3",
   "metadata": {},
   "source": [
    "# Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"IDH según datos del 2019\", \"IDH según datos del 2021\"]] # si no los paso con doble corchete en el predic tira error.\n",
    "Y = df.Crecimiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=10) #validación 80%(train) 20%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train) # entrenamiento\n",
    "\n",
    "print(\"Coeficientes:\", regr.coef_) # valor de las betas\n",
    "Y_pred = regr.predict(X) # prediccion\n",
    "print(\"R cuadrado:\", r2_score(Y,Y_pred)) #R cuadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6dcf98",
   "metadata": {},
   "source": [
    "## Gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, color=\"black\")\n",
    "plt.plot(X, Y_pred, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa924ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
