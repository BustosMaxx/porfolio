{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c9e1ae",
   "metadata": {},
   "source": [
    "# Introducción a Beautiful Soup\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "En este vídeo voy a mostrarte de manera muy breve la documentación del paquete Beautiful Soup. Lo primero que vamos a hacer es buscar en Google directamente este paquete de Python. Aquí nos sale remarcada la documentación, que es el tutorial que te recomiendo que sigas si quieres aprender exhaustivamente cómo hacer un \"web scraping\" con una página web. Lo que vamos a entrar directamente a la página principal que, aunque no lo parezca, es esta. Aquí te ofrece soporte sobre cómo instalarlo, que básicamente es el mismo comando para todos los paquetes, y un poco más de información sobre en qué consiste este paquete. Yo te recomiendo directamente ir a la documentación. La documentación de este paquete es un extensísimo tutorial de todas las funciones que ofrece. Podemos ver desde un arranque al código HTML, cómo mostrarlo de una manera mucho más atractiva y todas las funciones que podemos encontrar para Beautiful Soup. Vas a ver, si entras en esta página web, que tienes aquí mucha información para ir hacia abajo en el código HTML, ir hacia arriba, ir hacia los lados, arriba y abajo, buscar elementos, texto. Te recomiendo que explores toda esta documentación con mucha calma. Aquí hay muchísima información y dominar todas estas funciones puede tomarte mucho tiempo. Aun así, vale mucho la pena entrar en esta página web, ya que todo está muy bien explicado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6307c",
   "metadata": {},
   "source": [
    "# Fundamentos básicos de código HTML\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Aprender correctamente los fundamentos de HTML podría llevarte semanas. No es el objetivo de este vídeo. Yo tampoco soy un gran experto, y lo que te voy a mostrar aquí es una guía básica de lo que necesitas saber para realizar un \"scraping\" en Python. Hemos entrado en la página web de la Wikipedia en castellano y lo primero que vamos a hacer es pulsar en el botón derecho e inspeccionar el código HTML de esta página. Aquí lo tenemos. Es todo lo que vemos aquí. Está todo dentro de un objeto acotado por estas flechitas hasta que cierra. Esta es la sintaxis básica para abrir un espacio donde vemos que define una clase y tiene algunos parámetros y al final de todo cierra y está a la misma altura. Vemos que todo lo demás está dentro de esta apertura y este cierre con contrabarra y HTML. ¿Qué es lo que vamos a encontrar dentro? Pues más estructuras parecidas. Aquí, por ejemplo, tenemos todo el contenido de la página. Vamos a algo un poco más específico. Y Chrome funciona muy bien, ya que nos está subrayando en todo momento el espacio que ocupa. Por ejemplo, aquí tenemos un objeto que de identificador se llama 'firstHeading' y que, si seleccionamos, vemos el espacio que ocupa en nuestra pantalla. Este espacio de aquí. ¿Qué tenemos dentro? Dentro indexado vemos Python. Cuando realicemos un \"scraping\", lo que vamos a hacer es buscar estas etiquetas de aquí: 'h1' o 'div' o 'a'. Hay muchas etiquetas distintas. O buscar identificadores. Por ejemplo, 'bodyContent', que es el contenido de la página. O buscar por clases. Y vamos a tener que estar especificando constantemente qué es lo que queremos. Si lo que queremos es el 'bodyContent', vamos a tener que explorar posteriormente todo lo que hay dentro. Y todo lo que hay dentro van a ser los hijos de esta entrada de aquí. ¿Cuáles son sus padres? Esta de aquí. ¿Y sus hijos? Estábamos en esta, pues todas estas partes. Una buena manera de encontrar un elemento es directamente ir, por ejemplo, aquí si lo que nos interesa es escoger la información de la barra lateral, Inspeccionar, y nos va a remarcar dónde se encuentra en la estructura en árbol en HTML y cómo podríamos extraer esta información de aquí. Esto es lo más importante de tener controlado cuando estamos haciendo un \"scraping\": cuál es su posición en el árbol y bajo qué etiquetas podemos encontrarlo. Una de las informaciones más interesantes que podemos encontrar son los enlaces. Me gustaría saber, al final de todo tenemos la lista de bibliografía, y ver, por ejemplo, este enlace de aquí. Inspeccionamos y vemos que aquí está bajo la etiqueta 'a', que tiene una cierta clase. Aquí tiene una referencia, esta referencia es el enlace, el enlace que te va a llevar a otra página web. Extraer este tipo de información es la que te va a permitir hacer que tu \"scraping\" salte de página web en página web. Eso es muy importante. Y también es muy importante considerar que hay páginas de web externas, como esta de aquí, que te está redirigiendo fuera de la Wikipedia, o estas de aquí que, por ejemplo, no te están llevando fuera de la Wikipedia, sino que su enlace es una extensión de 'wikipedia.com' o 'wikipedia.es' y vas a tener que extraer esto y juntarlo con el enlace original. Esto obviamente solo es una introducción, pero permite poner en contexto un poco qué tipo de información estamos moviendo y cómo vamos a tener que procesarla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7925ff2",
   "metadata": {},
   "source": [
    "# Estructura tu web para usar Web scraping\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Obtener información útil de una página web utilizando Beautiful Soup es mucho más sencillo de lo que pueda parecer a priori. Lo primero que vamos a tener que hacer es cargar nuestro paquete. Este es 'BeautifulSoup', vas a tener que instalarlo si no lo tienes, y 'requests'. 'Requests' básicamente lo que va a hacer es permitirnos descargar una página web en concreto. Va a descargar el código HTML de la página que le hayamos pedido. Después lo que vamos a tener que hacer es, utilizando la función 'BeautifulSoup', convertir el contenido de esta página utilizando un transformador. Vamos a imprimir del objeto 'soup' una versión agradable, y aquí podemos ver directamente la estructura en árbol de HTML que acabamos de descargar. Tenemos una parte de código bastante fea al principio. Y después ya empezamos a ver toda la estructura de código y cómo están anidados los elementos unos dentro de otros. Este tipo de estructura es el que va a permitir que exploremos hacia adentro y hacia afuera todo el contenido de la página web. Voy a aprovechar esta introducción para introducir algunos de los elementos que pueden interesarnos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b62f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c93eb0",
   "metadata": {},
   "source": [
    "Esta instrucción de aquí, por ejemplo, lo que va hacer es mostrarnos exactamente lo mismo que antes, pero sin tener en cuenta la anidación de cada uno de los elementos. A mí personalmente me parece más útil la primera, ya que permite ver en qué nivel estamos y, como cierran los elementos, es más visual, aunque puede interesarnos ver todo bastante más compacto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba140a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e96efde",
   "metadata": {},
   "source": [
    " Hay algunos elementos a los que podemos acceder de manera directa. Por ejemplo, el título. Nos va a dar 'Python - Wikipedia, la enciclopedia libre'. Aquí lo vemos con sus etiquetas y con su cierre. Y lo que podemos hacer es, separando con puntos, buscar, por ejemplo, sus padres. En este caso, su padre. ¿Y qué tenemos? Tenemos la parte de código del principio, que es todo donde cuelga el título en sí. Estamos obteniendo la parte donde este título está anidado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in soup.title.children:\n",
    "    print(child)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6257e",
   "metadata": {},
   "source": [
    "Si lo que queremos es ver los hijos del título, es decir, todo lo que cuelga del título, tendríamos que ejecutar este \"loop\", ya que, si solamente ejecutamos esto, vamos a ver que no obtenemos lo esperado. Tenemos un iterador. Lo que interesa en realidad es ver cuál es el contenido. en este caso solo tiene un hijo, que es directamente el contenido del título, que es lo que nos interesaría si estamos extrayendo información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d248242",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4c418",
   "metadata": {},
   "source": [
    " Estamos obteniendo información mediante las etiquetas. Aquí, por ejemplo, podemos buscar la etiqueta 'a'. Si lo hacemos así, igual que con el título, lo que vamos a encontrar es la primera vez que encuentre 'a'. Esto no necesariamente implica que solo aparezca una vez. De hecho, puede confundirnos porque podríamos esperar que aparezcan todas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ac518",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f41b34",
   "metadata": {},
   "source": [
    "La instrucción para que aparezcan todas es esta de aquí. Aquí solamente nos está mostrando la primera de todas, que es 'top'. ¿Qué podemos ver si ejecutamos esto? Una lista de todos los componentes HTML que tienen una 'a' al principio. Da la casualidad, de hecho no es una casualidad, de que cuando encontramos 'a', lo que normalmente va asociada a ellas son enlaces a otras páginas, enlaces internos como este caso o enlaces externos como los que podríamos encontrar al final de todo en los enlaces a otros contenidos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d90e19",
   "metadata": {},
   "source": [
    " Y ya para terminar, podemos encontrar los componentes que estén etiquetados con una 'p'. Y básicamente aquí lo que vamos a encontrar son todas aquellas partes donde hay texto. Esto es especialmente importante si lo que queremos es obtener información para posteriormente realizar un análisis de \"text mining\". Aquí vemos todos los párrafos. Estos códigos que he mostrado aquí no son para nada exhaustivos. Aún hay muchas más funciones con las cuales podemos obtener información de un HTML, pero permite ver qué tipo de estructura podemos estar usando para obtener la información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad498d85",
   "metadata": {},
   "source": [
    "# Obtener información a través del HTML\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "A continuación, vamos a ver cómo podemos usar Beautiful Soup para obtener una cierta información de una página web. Aquí, por ejemplo, vamos a ir a buscar la información correspondiente a las referencias externas. Vamos a intentar obtener la información que aparece aquí y la que aparece aquí. Más concretamente conseguir los enlaces para ir siguiendo todas estas páginas web. Para hacerlo, vamos a cargar esta página, vamos a crear el 'soup' y vamos a mostrar la estructura en árbol. Lo primero que vamos a intentar, y no todo este código va a funcionar correctamente, es encontrar los enlaces externos, que es el título bajo el cual se encontraban algunos de los enlaces. Aquí sencillamente nos muestra el título. Esto no es lo que estábamos buscando. Así que vamos a ir a buscar todos los enlaces. Vamos a encontrar todos aquellos que cuenten con enlaces externos. Y vemos que volvemos a encontrarnos con lo mismo. Podemos usar las funciones de encontrar el elemento siguiente para ver cómo funciona y obtenemos esta información de aquí. Esto es un enlace, pero usaré esta estructura de aquí. Podríamos usarla para ir iterando sobre cuál es el elemento siguiente. Pero no termina de ser la manera más elegante de buscar toda la información que nos interesa. Otra opción que propongo es explorar la función contenido, y vemos que el contenido es sencillamente toda esta parte de aquí que no es código de HTML, que es exactamente esto. Estas aproximaciones que usan las funciones más básicas de Beautiful Soup son bastante más limitadas que realizar un análisis un poco más en profundidad. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ada0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fe872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"Enlaces_externos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755feb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces = soup.find(id=\"Enlaces_externos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d489f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25911337",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544080e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces.find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6b424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a899a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb96dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces = soup.findAll(\"div\",{\"class\":\"listaref\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enlaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752e15c",
   "metadata": {},
   "source": [
    "Es lo que te propongo a continuación. Primero vamos a ver cómo podemos encontrar todos aquellos elementos que estén categorizados con esta etiqueta y que pertenezcan a una cierta clase. En este caso, el listado de referencias. Aquí obtenemos algo que tiene bastante mejor pinta que lo que estábamos obteniendo. Pero, aún así, si lo que nos interesa son los enlaces, aquí aún hay demasiado código. Por ejemplo, vemos que bajo esta etiqueta tenemos enlaces, que es lo que nos interesa conseguir, pero aún hay la referencia, dónde la cita, también hay el nombre y avisos, por ejemplo. Todo esto es información que no queremos, y lo hemos obtenido con esta instrucción de aquí. Y lo que vamos a ver es otro método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6ec14",
   "metadata": {},
   "source": [
    "Vamos a ver este de aquí primero. Aquí estamos consiguiendo todos los objetos que cuelguen de esta etiqueta de aquí. Esto he ido a explorarlo usando el inspeccionar en la página web original. No son etiquetas que sean globales, sino que esto depende en concreto de cómo está estructurada la Wikipedia en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in soup.find(\"div\", attrs={\"class\":\"listaref\"}).descendants:\n",
    "    if d.name == \"span\" and d.get(\"class\", \"\") == [\"reference-text\"]:\n",
    "        print(d.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb631a3",
   "metadata": {},
   "source": [
    " Y este código de aquí es más elaborado y lo que estamos encontrando es cada uno de los elementos que encuentre nuestra etiqueta que cumplan, que sean de la clase lista de referencias, buscar todos sus descendientes. Una vez dentro de esta lista, por cada uno de estos elementos vamos a buscar aquellos cuyo nombre sea este, 'span', y que tengan esta clase, que pertenezcan a este grupo. Y vamos a imprimirlos. Aquí ya hemos obtenido algo mucho más útil. Estamos en el apartado de referencias, pero solo estamos obteniendo los \"links\" de aquellas páginas que directamente tenían el \"link\" en la página web. Esto eran enlaces y aquí no los estamos obteniendo. Solamente estamos obteniendo aquellos que directamente estaban escritos. Es decir, este código funciona correctamente, estamos obteniendo la información de las referencias, pero estamos obteniendo directamente el título de la referencia, no el enlace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eef5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find(\"div\", attrs={\"class\":\"listaref\"})\n",
    "for div in data:\n",
    "    links = div.findAll(\"a\")\n",
    "    for a in links:\n",
    "        if a[\"href\"].startswith(\"http\"):\n",
    "            print(a[\"href\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61f3e0",
   "metadata": {},
   "source": [
    " Y para eso os he puesto aquí un último código, que lo que obtenemos es todos los datos que estén etiquetados así, que pertenezcan a esta clase y, para cada una de estas, vamos a encontrar todos los que estén categorizados con 'a'. 'a' normalmente va asociado a 'links', es decir, enlaces. Y por cada uno de los que haya encontrado aquí, por cada 'a' que encuentre en este objeto, si la hiperreferencia de 'a' empieza con 'http', vamos a imprimirlo. Este código puede parecer muy complicado, pero en realidad solo estamos explorando cada uno de los elementos y, dentro de cada uno de los elementos, cada uno de los elementos. Si ejecutamos esto, obtenemos exactamente lo que estábamos buscando, que es todos los enlaces para cada una de las referencias que teníamos en la página web original. Este tipo de código de aquí está combinando funciones propias de Beautiful Soup con \"loops\" mediante los cuales vamos explorando todos los objetos que nos devuelven estas funciones. Puede parecer muy complejo al principio porque estás trabajando con objetos que son bastante distintos a los que estás acostumbrado con Python. Pero en realidad todo puede explorarse como si fuesen listas. Y ya ves que no estoy utilizando más que dos funciones en particular. Todo lo demás son tratamiento de \"strings\", bucles y condicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d85aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find(\"div\", attrs={\"class\":\"listaref\"})\n",
    "#link = data.findAll(\"a\")\n",
    "for div in data:\n",
    "    links = div.findAll(\"a\")\n",
    "    for a in links:\n",
    "        if a[\"href\"].startswith(\"http\"):\n",
    "            print(a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe61a3",
   "metadata": {},
   "source": [
    "Y para eso os he puesto aquí un último código, que lo que obtenemos es todos los datos que estén etiquetados así, que pertenezcan a esta clase y, para cada una de estas, vamos a encontrar todos los que estén categorizados con 'a'. 'a' normalmente va asociado a 'links', es decir, enlaces. Y por cada uno de los que haya encontrado aquí, por cada 'a' que encuentre en este objeto, si la hiperreferencia de 'a' empieza con 'http', vamos a imprimirlo. Este código puede parecer muy complicado, pero en realidad solo estamos explorando cada uno de los elementos y, dentro de cada uno de los elementos, cada uno de los elementos. Si ejecutamos esto, obtenemos exactamente lo que estábamos buscando, que es todos los enlaces para cada una de las referencias que teníamos en la página web original. Este tipo de código de aquí está combinando funciones propias de Beautiful Soup con \"loops\" mediante los cuales vamos explorando todos los objetos que nos devuelven estas funciones. Puede parecer muy complejo al principio porque estás trabajando con objetos que son bastante distintos a los que estás acostumbrado con Python. Pero en realidad todo puede explorarse como si fuesen listas. Y ya ves que no estoy utilizando más que dos funciones en particular. Todo lo demás son tratamiento de \"strings\", bucles y condicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af5d87",
   "metadata": {},
   "source": [
    "---\n",
    "# Extracción de texto usando Beautiful Soup\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Si quieres extraer información textual de una página web para posteriormente realizar un análisis de \"text mining\", en este vídeo voy a mostrarte distintas maneras de cómo puedes hacerlo. Lo primero que tendremos que hacer es cargar toda la información del HTML realizando estas dos funciones y vamos a visualizar qué tipo de información tenemos. Tenemos la estructura en árbol del HTML de la Wikipedia en la entrada de Python. Y aquí vamos a intentar extraer todo lo que corresponde al cuerpo del artículo, es decir, al texto en sí. Una primera aproximación es usar este tipo de estructuras. Por ejemplo, voy a usar el 'sub', voy a extraerle el título y convertirlo en 'string'. Esto funciona correctamente, pero esto solamente es el título. Beautiful Soup nos ofrece esa función que es bastante útil, pero no funciona todo lo exhaustivamente que querríamos. Efectivamente coge texto, pero también nos incorpora algunas cosas que no nos interesan. Por ejemplo, esto claramente no es el texto que estábamos buscando. Más adelante sí que hay texto. Nos incluye también el índice, que no es especialmente interesante si lo que queremos es información textual para procesar. Y aquí ya sí que tenemos más información como la que nos interesa, pero aún hay elementos que no nos interesan. Otra aproximación es ir a buscar el cuerpo del artículo. Aquí tenemos directamente todo el cuerpo con el código asociado. Y podríamos intentar explorar, por ejemplo, los elementos categorizados con 'p'. Es lo que vemos aquí abajo. Obtenemos información. Ciertamente esto es la información que estamos buscando procesar, pero aún no está limpia. Requeriría aún mucho procesamiento. Vamos a hacerlo guardando esta instrucción de aquí. Todo lo que encuentres dentro del cuerpo, todos los elementos categorizados como 'p' y vamos a ver qué tenemos aquí. Tenemos en una lista cada uno de los elementos donde el programa ha detectado que tenemos un párrafo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61962eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos = soup.find(\"body\").find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpiarhtml(html):\n",
    "    expresion = re.compile('<.*?>')\n",
    "    texto = re.sub(expresion, \"\", html)\n",
    "    return texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83987255",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in elementos:\n",
    "    print(limpiarhtml(str(elem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e031e6b",
   "metadata": {},
   "source": [
    "Vamos a importar un nuevo paquete y a definir una función que va a limpiarnos todo lo que es código HTML. ¿Cómo se limpia el código HTML? Es muy sencillo. Tenemos que decirle que todo lo que encuentre entre estas flechitas nos lo extraiga. Como es una página web, todo esto va a eliminarlo. En principio, no tendría que haber ningún problema, porque una página web no compilaría si uno de estos está abierto pero no cerrado. Así que, lo que vamos a hacer es pedirle que nos extraiga esto y lo sustituya por nada y directamente nos devuelva el texto limpio. Y aquí sencillamente tenemos que ejecutar esta función sobre cada uno de los elementos que hayamos encontrado usando esta función de aquí. Vamos a ver el resultado. Esto es exactamente lo que estábamos buscando, es decir, toda la información textual lista para ser procesada con un modelo. Es importante notar que he tenido que convertir esto a un 'string'. Si no lo hago, Python se va a quejar. Estaba esperando un 'string' y tengo otro tipo de objeto. Es tan sencillo como añadirle aquí esta conversión. El código para obtener la información textual, como has visto, no es especialmente complicado. Sencillamente es darse cuenta de dónde se encuentra, bajo qué etiqueta y definir una función que puedes usarla, y es muy sencilla de crear, para limpiar todo aquello que no nos interesa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23b7ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Obtención de información no textual\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Una de las vertientes del \"web scraping\" que más interesantes me parecen personalmente es la obtención de \"links\" para explorar cómo están conectadas las páginas entre ellas. Aquí lo que te voy a mostrar son distintas maneras de obtener estos \"links\" con códigos bastante sencillos. Voy a estar usando esta página web, la entrada de la Wikipedia de Python, y vamos a crear un 'soup' directamente. Y vamos a ver cómo obtener todos los \"links\" de una manera muy sencilla. Primero es, dentro del 'soup', encontrar todos los elementos que estén categorizados con 'a' y que tengan atributo 'hiperreferencia', que cumplan que empieza por 'http'.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40692f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276460e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.findAll(\"a\", attrs={\"href\": re.compile(\"^http://\")}):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca7588",
   "metadata": {},
   "source": [
    " ¿Qué estamos obteniendo con esto? No estamos obteniendo todos los \"links\", estamos obteniendo todos los \"links\" que apunten hacia fuera de la página web. Vamos a ver exactamente qué es lo que estamos obteniendo. Y son todos los enlaces que llevan a otras páginas. Aquí no hay entradas de la Wikipedia. Esto es una primera aproximación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b23e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselink = \"https://es.wikipedia.org\"\n",
    "\n",
    "for link in soup.findAll(\"a\"):\n",
    "    if str(link.get(\"href\"))[0] == \"/\":\n",
    "        print(baselink + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b601b76",
   "metadata": {},
   "source": [
    "A continuación, te voy a mostrar una manera de obtener los \"links\" que apunten solamente dentro de la Wikipedia. ¿Qué es lo que tenemos que hacer? Tenemos que definir cuál es la web dentro de la cual estamos para obtener el incompleto y sencillamente lo que vamos a hacer es buscar todos los elementos que estén categorizados como 'a'. Esto es lo mismo que estábamos haciendo antes. Y, si el \"link\" empieza con una contrabarra –hay muchas maneras de hacer esto, yo he usado esta–, sencillamente que la primera posición en nuestros \"links\" sea una contrabarra, imprímeme el enlace original, la base, con el \"link\" que encuentres. ¿Por qué estoy haciendo esto? Porque el \"link\" que encuentra no es el \"link\" completo, sino que es la extensión de este \"link\" original. Vamos a ver que los \"links\" que obtenemos tienen sentido. Lo que está obteniendo esta parte de aquí es sencillamente esto de aquí. Si intento obtener la información solamente con este enlace, me dirá que esta página web no existe o sencillamente ni lo detectará como una página web. Este código funciona muy bien, pero hay algunas excepciones en las que no funciona. Estoy hay que explorarlo normalmente a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselink = \"https://es.wikipedia.org\"\n",
    "\n",
    "for link in soup.findAll(\"a\"):\n",
    "    if str(link.get(\"href\"))[0] == \"/\" and str(link.get(\"href\"))[1] != \"/\":\n",
    "        print(baselink + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d3ba0",
   "metadata": {},
   "source": [
    "Es decir, ver si funciona y todo lo que estás obteniendo tiene sentido. Y vamos a ver algún ejemplo en el que no. Aquí, por ejemplo, nos está redirigiendo a unas páginas que tienen dos contrabarras. Esto no promete en general, porque aunque sean páginas que cuelguen de la Wikipedia, como vemos, Wikimedia Foundation, esto no tiene este camino. Si pulsamos en este \"link\", se nos va a abrir una página que dice que esto no existe y nos dice el \"link\" de verdad. Básicamente tenemos que poner '/wiki//' y lo que ha obtenido. ¿Cómo podemos evitar conseguir estos \"links\" que siguen una estructura distinta y que en realidad nos está llevando a enlaces que no son los que queremos? Podemos hacerlo con este código de aquí. Este tipo de procesos están basados en el ensayo y el error. Aquí yo no había contemplado la posibilidad de que existiera este tipo de estructura. ¿Cómo lo añado? Sencillamente, en la condición, busco que el primero sea una contrabarra, pero busco que el segundo no lo sea. Así me ahorro que el programa detecte este tipo de \"links\" y todos los enlaces que está consiguiendo ahora ya no siguen esta estructura que no funcionaba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d3f3f",
   "metadata": {},
   "source": [
    "---\n",
    "# Creación de un spider o crawler\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Un \"spyder\" o \"crawler\" es el tipo de programa, en este caso de Python, que lo que va a hacer es explorar la web por nosotros automáticamente. ¿A qué me estoy refiriendo exactamente? A un programa que va a coger un enlace, el enlace inicial que nosotros le demos, va a obtener un conjunto de \"links\", es decir, todos los \"links\" internos, por ejemplo. En este ejemplo vamos a ver los \"links\" internos dentro de la Wikipedia. Y va a ir saltando por estos \"links\" buscando más \"links\" y realizando algún análisis. Se le llama \"spyder\" porque es una araña que va por toda la red y automáticamente va entrando en cada uno de los enlaces que tú le estás dando. Esto hay que programarlo. El código no es sencillo, pero tampoco son muchísimas líneas y el potencial que tiene es ilimitado. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5065dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, re, time\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "listalinksexplorados = set()\n",
    "listalinksporexplorar = set()\n",
    "\n",
    "\n",
    "\n",
    "baselink = \"https://es.wikipedia.org\"\n",
    "\n",
    "for link in soup.findAll(\"a\"):\n",
    "    if str(link.get(\"href\"))[0] == \"/\" and str(link.get(\"href\"))[1] != \"/\":\n",
    "        print(baselink + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44dd0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.findAll(\"a\"):\n",
    "    if str(link.get(\"href\"))[0] == \"/\" and str(link.get(\"href\"))[1] != \"/\" and baselink + link.get(\"href\") not in listalinksexplorados: \n",
    "        listalinksporexplorar.add(baselink + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "listalinksporexplorar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88146151",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es crear el 'soup'. Y estos dos objetos son muy importantes. Los he llamado directamente por lo que son. Esto es una lista de los \"links\" que ya hemos explorado, esto es importante, porque no queremos estar explorando constantemente las mismas páginas. Y esta es una lista de los \"links\" que aún no hemos explorado. Vamos a explorar los \"links\" que cuelguen de esta página web. Y aquí tenemos una primera lista de los \"links\" que encontramos en la página web de Python de la Wikipedia. Lo que voy a hacer es adaptar este código para que llene la lista de \"links\" por explorar. ¿Qué estamos haciendo? Exactamente lo mismo que antes, es decir, estamos cogiendo todos los \"links\" que sean relativos, es decir, que empiecen por una contrabarra. Es una buena manera de hacerlo. Que la segunda no sea una contrabarra esto es una excepción concreta de las Wikipedias, que hay algunos \"links\" que tienen dos contrabarras y luego ya no puede accederse a ellos. Y otra condición que antes no estaba es que el \"link\" conjunto no esté en la lista de los \"links\" que ya hemos explorado. No lo he comentado antes, pero si te has fijado, estos dos objetos de aquí no son listas, de hecho. Son sets. ¿Cuál es la diferencia entre un set y una lista? Que un set no admite elementos repetidos. Hay más diferencias, pero esta es la que aprovecharemos. Y así nos estamos ahorrando tener que filtrar si tenemos el mismo \"link\" varias veces. Si ejecutamos este código, estamos llenando una primera lista de \"links\" por explorar. Tenemos todo esto. Haría falta limpiarlo. Aquí vemos que tenemos unos cuantos \"links\" que sencillamente nos redirigen a otras secciones. No voy a entrar en esto ahora mismo, aunque tampoco sería especialmente complicado si sabemos la estructura que tiene que tener un \"link\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50785f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(len(listalinksporexplorar) != 0):\n",
    "    link = listalinksporexplorar.pop()\n",
    "    pagina = requests.get(link)\n",
    "    soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "    print(\"Links Explorados: \", len(listalinksexplorados), \" Links por Explorar: \", len(listalinksporexplorar))\n",
    "    listalinksexplorados.add(link)\n",
    "    time.sleep(1)\n",
    "    for link in soup.findAll(\"a\"):\n",
    "        if str(link.get(\"href\"))[0] == \"/\" and str(link.get(\"href\"))[1] != \"/\" and baselink + link.get(\"href\") not in listalinksexplorados: \n",
    "            listalinksporexplorar.add(baselink + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listalinksexplorados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb75edb",
   "metadata": {},
   "source": [
    "Y este es el código que me interesa comentar. Como vamos a estar variando el tamaño de esta lista, que en realidad es un set, no podemos hacer un bucle con un 'for', sino que tenemos que hacer un 'while' que funcione mientras esta lista de \"links\" por explorar no sea cero. También podríamos añadirle una condición para que el número total de \"links\" explorados sea 1000, 10 000... Los que queramos. Aquí extraemos un \"link\" de estos \"links\" por explorar y realizamos el mismo análisis que hacíamos al principio. Cogemos la página, le pedimos que haga un 'soup' y nos va a imprimir por pantalla cuántos \"links\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64dcc4",
   "metadata": {},
   "source": [
    "---\n",
    "# Desafío: Web scraping y modelización\n",
    "Si seleccionas líneas de la transcripción en esta sección, irás a la marca de tiempo en el vídeo\n",
    "Para el desafío final de este curso, te voy a pedir que hagas un \"web scraping\" para la tabla que estás viendo por pantalla. Lo que te voy a pedir que consigas es esta lista de países, incluyendo la media general, y estas cuatro columnas de aquí. Hay muchísimas maneras distintas de hacerlo. Lo que me interesa es que consigas todos estos valores para posteriormente ajustar un modelo de \"machine learning\" con ellos. Intenta que tu código sea reproducible si se añaden nuevos años al cabo de un tiempo. Una vez hayas conseguido los datos, te voy a pedir que hagas un modelo para predecir este valor basándote en los tres anteriores. Y, para terminar, te voy a pedir que pienses cómo lo harías para predecir el valor de 2018 aprovechando el modelo que acabas de generar. Mucha suerte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pagina = requests.get(\"https://es.wikipedia.org/wiki/Anexo:Países_de_América_Latina_por_índice_de_desarrollo_humano\")\n",
    "soup = BeautifulSoup(pagina.content, \"html.parser\")\n",
    "\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_IDH =soup.find(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc59948",
   "metadata": {},
   "source": [
    "## Encabezados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_head = tabla_IDH.find_all(\"th\")\n",
    "header = [i.string[:-1] for i in lista_head] \n",
    "encabezado = header[1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d63ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encabezado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e625f",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = tabla_IDH.find_all(\"td\")\n",
    "lista_datos = [i.get_text()[:-1] if \"\\n\" in i.get_text() else i.get_text() for i in aux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daa969",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = np.array(lista_datos)\n",
    "valores.shape = (20, len(encabezado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf73b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Puesto = [i[0] for i in valores]\n",
    "Puesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(valores, index=Puesto, columns=encabezado)\n",
    "del df[\"Puesto\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IDH según datos del 2021\"] = df[\"IDH según datos del 2021\"].apply(str.strip)\n",
    "df[\"IDH según datos del 2021\"] = df[\"IDH según datos del 2021\"].str.replace(\",\", \".\")\n",
    "df[\"IDH según datos del 2021\"] = df[\"IDH según datos del 2021\"].astype(\"float64\")\n",
    "df[\"IDH según datos del 2019\"] = df[\"IDH según datos del 2019\"].apply(str.strip)\n",
    "df[\"IDH según datos del 2019\"] = df[\"IDH según datos del 2019\"].str.replace(\",\", \".\")\n",
    "df[\"IDH según datos del 2019\"] = df[\"IDH según datos del 2019\"].astype(\"float64\")\n",
    "df[\"Crecimiento\"] = df[\"Crecimiento\"].apply(str.strip)\n",
    "df[\"Crecimiento\"] = df[\"Crecimiento\"].str.replace(\",\", \".\")\n",
    "df[\"Crecimiento\"] = df[\"Crecimiento\"].astype(\"float64\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6160c3",
   "metadata": {},
   "source": [
    "# Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"IDH según datos del 2019\", \"IDH según datos del 2021\"]] # si no los paso con doble corchete en el predic tira error.\n",
    "Y = df.Crecimiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=10) #validación 80%(train) 20%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train) # entrenamiento\n",
    "\n",
    "print(\"Coeficientes:\", regr.coef_) # valor de las betas\n",
    "Y_pred = regr.predict(X) # prediccion\n",
    "print(\"R cuadrado:\", r2_score(Y,Y_pred)) #R cuadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6dcf98",
   "metadata": {},
   "source": [
    "## Gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, color=\"black\")\n",
    "plt.plot(X, Y_pred, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa924ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
